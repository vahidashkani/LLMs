# -*- coding: utf-8 -*-
"""LLM_automatic_prompt_engineer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kuy7Yjq5izSvWkrrqmUbyl3ErfQAVm_8

# **Notes**

This notebook showcases how to leverage APE to automate and optimize the prompt engineering process. By comparing APE-generated prompts with a human-written prompt, you can observe how automated prompt engineering can enhance or match manually designed prompts for text generation tasks.
Requirements

    Python 3.6+
    transformers library
    OpenAI API key
"""

# # Optimizing Prompts with Automatic Prompt Engineer (APE)

# This notebook demonstrates how to use Automatic Prompt Engineer (APE) to optimize prompts for text generation.
# APE leverages language models to automatically design and evaluate prompts, generating outputs that match a provided dataset.

# APE uses a two-step process:
# 1. **Prompt Generation**: A language model generates multiple candidate prompts.
# 2. **Prompt Evaluation**: A scoring function evaluates each prompt’s effectiveness in generating desired outputs.

# The best-scoring prompt is then returned as the optimized prompt.

# ### References:
# * [Automatic Prompt Engineer (APE) on GitHub](https://github.com/keirp/automatic_prompt_engineer)
# * [Paper on arXiv](arxiv_link)

# ## 1. Install Dependencies

# To start, we need to install the Automatic Prompt Engineer package directly from GitHub.

!pip install git+https://github.com/keirp/automatic_prompt_engineer

# ## 2. Import Required Libraries
# We'll import `openai` for API access and `ape` for prompt engineering.

import openai
from automatic_prompt_engineer import ape

# Set up your OpenAI API key.
openai.api_key = ''  # Replace with your actual API key

# ## 3. Define the Dataset
# We'll use a simple dataset of words and their antonyms to test APE's prompt optimization.

# List of words and their antonyms to act as input-output pairs.
words = [
    "sane", "direct", "informally", "unpopular", "subtractive", "nonresidential",
    "inexact", "uptown", "incomparable", "powerful", "gaseous", "evenly",
    "formality", "deliberately", "off"
]
antonyms = [
    "insane", "indirect", "formally", "popular", "additive", "residential",
    "exact", "downtown", "comparable", "powerless", "solid", "unevenly",
    "informality", "accidentally", "on"
]

# ## 4. Define the Prompt Template
# The prompt template provides a structure for how prompts are generated. APE will use this template during optimization.

# Template for the evaluation process.
eval_template = \
"""Instruction: [PROMPT]
Input: [INPUT]
Output: [OUTPUT]"""

# ## 5. Generate Optimized Prompts with APE

# APE will now attempt to find prompts that accurately generate antonyms for each word in our dataset.
# It does so by using the provided template and evaluating multiple candidate prompts.

result, demo_fn = ape.simple_ape(
    dataset=(words, antonyms),      # Input-output pairs
    eval_template=eval_template,    # Evaluation template
)

# Display the optimized prompt.
print("Optimized Prompt Result:", result)

# ## 6. Comparison with a Manually Written Prompt
# To gauge the effectiveness of APE, we’ll compare its generated prompt with a manually written one.

# Define a manually written prompt to generate antonyms.
manual_prompt = "Write an antonym to the following word."

# Use the manual prompt for evaluation and compare it with the APE-generated prompt.
human_result = ape.simple_eval(
    dataset=(words, antonyms),
    eval_template=eval_template,
    prompts=[manual_prompt],
)

# Display the result of the manual prompt for comparison.
print("Manual Prompt Result:", human_result)