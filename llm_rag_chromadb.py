# -*- coding: utf-8 -*-
"""LLM_RAG_chromaDB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kuy7Yjq5izSvWkrrqmUbyl3ErfQAVm_8

# **Notes**

This notebook provides a complete example of creating a RAG pipeline with LangChain and ChromaDB. With this setup, we can leverage external knowledge sources, such as PDF documents, and use embeddings to enrich the LLM's responses. This process not only improves answer quality but also ensures answers are based on reliable source content.
Requirements

    Python 3.6+
    langchain, sentence-transformers, pdfplumber, chromadb, torch, transformers libraries
    OpenAI API key (for external LLM calls if needed)
"""

# # Building a Retrieval-Augmented Generation (RAG) Pipeline with LangChain

# This notebook demonstrates how to create a RAG pipeline with LangChain, Sentence Transformers, and ChromaDB.
# The pipeline can handle both standalone language model (LLM) questions and question-answering tasks using an external knowledge source.
# Here’s an overview:
# - We load embeddings and language models for efficient text processing.
# - Use a PDF document as a knowledge source.
# - Split and embed text from the PDF and save it to ChromaDB.
# - Set up a Retrieval-Augmented Generation (RAG) framework to retrieve relevant context for improved answers.

# ## 1. Install Dependencies

# Begin by installing the necessary libraries to set up the RAG pipeline.

!pip install langchain-community
!pip install langchain
!pip install sentence-transformers
!pip install pdfplumber
!pip install chromadb
!pip install tiktoken

# ## 2. Import Required Libraries

# Import libraries for building embeddings, loading PDF documents, and setting up the RAG pipeline.
import torch
from transformers import AutoTokenizer, pipeline
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.document_loaders.pdf import PDFPlumberLoader
from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter
from langchain.vectorstores import Chroma
from langchain.llms.huggingface_pipeline import HuggingFacePipeline
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# ## 3. Create Pipelines for Embedding and Text Generation

# We define two functions: one for generating embeddings and another for text generation with a transformer model.

# ### Function to Generate Sentence-BERT Embeddings with MPNet
def create_sbert_mpnet():
    model = "sentence-transformers/all-mpnet-base-v2"
    device = "cuda" if torch.cuda.is_available() else "cpu"
    return HuggingFaceEmbeddings(model_name=model, model_kwargs={"device": device})

# ### Function to Create FLAN-T5 Text-Generation Pipeline
def create_flan_t5_base(load_in_8bit):
    model = "google/flan-t5-xl"
    tokenizer = AutoTokenizer.from_pretrained(model)
    # Wrapping model in Hugging Face pipeline to use with LangChain
    return pipeline(
        task="text2text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=100,
        device_map="auto",
        model_kwargs={"load_in_8bit": load_in_8bit, "temperature": 0.95}
    )

# Instantiate embedding and language model pipelines
embedding = create_sbert_mpnet()
llm = create_flan_t5_base(load_in_8bit=False)

# ## 4. Using the LLM for Direct Question Answering (No RAG)

# Define a sample question and prompt template for direct text generation without RAG.

_question = "What is Grant of Patent in ArangoDB License?"
prompt = f"Answer the following question:\n{_question}"

# Generate a response using the language model.
response = llm(prompt)
print("LLM Response:", response)

# ## 5. Load and Process a PDF Document as Knowledge Source

# Define a PDF document URL and load it with `PDFPlumberLoader`.
# We’ll split the document into chunks for storage in ChromaDB.

pdf_path = "https://arangodb.com/wp-content/uploads/2023/09/Download-Contributor-License-Agreement.pdf"
loader = PDFPlumberLoader(pdf_path)
documents = loader.load()

# Split document into chunks for efficient retrieval
text_splitter = TokenTextSplitter(chunk_size=200, chunk_overlap=10)
texts = text_splitter.split_documents(documents)

# Initialize ChromaDB with the text chunks and store the embeddings
persist_directory = "./chromadb"
vectordb = Chroma.from_documents(documents=texts, embedding=embedding, persist_directory=persist_directory)

# ## 6. Inspect Extracted Text Chunks

# Print the number of text chunks and display a few samples to verify.
print("Number of Text Chunks:", len(texts))
print("\nSample Text Chunks:")
for t in texts[:4]:
    print(t)
    print("#" * 50)

# ## 7. Inspect Data Stored in Chroma VectorDB

# Retrieve and print a few examples of documents and embeddings stored in ChromaDB.
vdb = vectordb.get(include=['embeddings', 'documents', 'metadatas'])
num_samples = 2
print("Sample Documents:", vdb['documents'][:num_samples])
print("Sample Embeddings:", vdb['embeddings'][:num_samples])
print("Sample Metadata:", vdb['metadatas'][:num_samples])

# ## 8. Setting up the Retrieval-Augmented Generation (RAG) Framework

# We will combine the language model with ChromaDB to retrieve relevant context for each question.

# Initialize the HuggingFace pipeline for use with LangChain
hf_llm = HuggingFacePipeline(pipeline=llm)

# Set up retriever with ChromaDB, retrieving the top relevant document
retriever = vectordb.as_retriever(search_kwargs={"k": 1})
qa = RetrievalQA.from_chain_type(llm=hf_llm, chain_type="stuff", retriever=retriever)

# Define a custom prompt template for FLAN models
question_t5_template = """
context: {context}
question: {question}
answer:
"""
QUESTION_T5_PROMPT = PromptTemplate(
    template=question_t5_template, input_variables=["context", "question"]
)
# Apply custom prompt template to RAG
qa.combine_documents_chain.llm_chain.prompt = QUESTION_T5_PROMPT

# ## 9. Running the RAG Framework for a Query

# Set up a verbose response to include source documents in the output.
qa.combine_documents_chain.verbose = True
qa.return_source_documents = True

# Run a sample query using RAG
rag_response = qa({"query": _question})
print("RAG Response:", rag_response)

# ## 10. Retrieve Relevant Documents for a Query

# Retrieve and print the most relevant documents for the given question.
retrieved_docs = retriever.get_relevant_documents(_question)
print("Retrieved Documents:", retrieved_docs)

# Invoke the retriever for context and format with the custom prompt.
context = retrieved_docs[0].page_content
formatted_prompt = QUESTION_T5_PROMPT.format(question=_question, context=context)
print("Formatted Prompt with Context:\n", formatted_prompt)